{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NPL Natural Disasters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-03 18:46:43.258053: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-03 18:46:43.292477: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "\n",
    "import collections\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import spacy\n",
    "# import scattertext as st\n",
    "\n",
    "from unidecode import unidecode\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    ")\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.metrics.distance import jaccard_distance\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import words\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"words\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "\n",
    "!python -m spacy download en_core_web_sm\n",
    "# !python -m spacy download en_core_web_trf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3258</th>\n",
       "      <td>10861</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FASTE...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3259</th>\n",
       "      <td>10865</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Storm in RI worse than last hurricane. My city...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3260</th>\n",
       "      <td>10868</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Green Line derailment in Chicago http://t.co/U...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3261</th>\n",
       "      <td>10874</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MEG issues Hazardous Weather Outlook (HWO) htt...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3262</th>\n",
       "      <td>10875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#CityofCalgary has activated its Municipal Eme...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10876 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1     NaN      NaN   \n",
       "1         4     NaN      NaN   \n",
       "2         5     NaN      NaN   \n",
       "3         6     NaN      NaN   \n",
       "4         7     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "3258  10861     NaN      NaN   \n",
       "3259  10865     NaN      NaN   \n",
       "3260  10868     NaN      NaN   \n",
       "3261  10874     NaN      NaN   \n",
       "3262  10875     NaN      NaN   \n",
       "\n",
       "                                                   text  target  train  \n",
       "0     Our Deeds are the Reason of this #earthquake M...     1.0      1  \n",
       "1                Forest fire near La Ronge Sask. Canada     1.0      1  \n",
       "2     All residents asked to 'shelter in place' are ...     1.0      1  \n",
       "3     13,000 people receive #wildfires evacuation or...     1.0      1  \n",
       "4     Just got sent this photo from Ruby #Alaska as ...     1.0      1  \n",
       "...                                                 ...     ...    ...  \n",
       "3258  EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FASTE...     NaN      0  \n",
       "3259  Storm in RI worse than last hurricane. My city...     NaN      0  \n",
       "3260  Green Line derailment in Chicago http://t.co/U...     NaN      0  \n",
       "3261  MEG issues Hazardous Weather Outlook (HWO) htt...     NaN      0  \n",
       "3262  #CityofCalgary has activated its Municipal Eme...     NaN      0  \n",
       "\n",
       "[10876 rows x 6 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load datasets\n",
    "\n",
    "df_train = pd.read_csv(\"data/train.csv\")\n",
    "df_train[\"train\"] = 1\n",
    "df_test = pd.read_csv(\"data/test.csv\")\n",
    "df_test[\"train\"] = 0\n",
    "df = pd.concat([df_train, df_test], axis=0)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform data cleaning\n",
    "\n",
    "# 1. convert all letters to lowercase, so capitalization does not influence word frequency\n",
    "df[\"text_cleaned\"] = df[\"text\"].str.lower()\n",
    "\n",
    "# 2. remove @mention\n",
    "df[\"text_cleaned\"] = df[\"text_cleaned\"].str.replace(\"@\\w+|.@\\w+\", \"\", regex=True)\n",
    "\n",
    "# 3. remove emoji\n",
    "# credit: https://stackoverflow.com/questions/33404752/removing-emojis-from-a-string-in-python\n",
    "df[\"text_cleaned\"] = df[\"text_cleaned\"].str.replace(\n",
    "    \"\\U0001F600-\\U0001F64F\", \"\", regex=True\n",
    ")\n",
    "df[\"text_cleaned\"] = df[\"text_cleaned\"].str.replace(\n",
    "    \"\\U0001F300-\\U0001F5FF\", \"\", regex=True\n",
    ")\n",
    "df[\"text_cleaned\"] = df[\"text_cleaned\"].str.replace(\n",
    "    \"\\U0001F680-\\U0001F6FF\", \"\", regex=True\n",
    ")\n",
    "df[\"text_cleaned\"] = df[\"text_cleaned\"].str.replace(\n",
    "    \"\\U0001F680-\\U0001F6FF\", \"\", regex=True\n",
    ")\n",
    "\n",
    "# 4. remove non alphabetical characters\n",
    "df[\"text_cleaned\"] = df[\"text_cleaned\"].str.replace(\"/[^\\x00-\\x7F]+/\", \"\", regex=True)\n",
    "\n",
    "# 4. remove special character referencing starting with &\n",
    "df[\"text_cleaned\"] = df[\"text_cleaned\"].str.replace(\"&[a-z]+;?\", \"\", regex=True)\n",
    "\n",
    "# 5. remove http and https links\n",
    "df[\"text_cleaned\"] = df[\"text_cleaned\"].str.replace(\n",
    "    \"((?:http|https):\\/\\/[-\\w\\d+=&@#\\/%?~|!:;\\.,]*)\", \"\", regex=True\n",
    ")\n",
    "\n",
    "# 6. remove punctuations with a space, except for apostrophes\n",
    "# note that many text do not have space after punctuations, to prevent joining separate words, replace with space\n",
    "df[\"text_cleaned\"] = df[\"text_cleaned\"].str.replace(\"[^\\w\\s']\", \" \", regex=True)\n",
    "\n",
    "# 7. translate non ascii text to closest representation\n",
    "# credit: https://stackoverflow.com/questions/517923/what-is-the-best-way-to-remove-accents-normalize-in-a-python-unicode-string\n",
    "df[\"text_cleaned\"] = df[\"text_cleaned\"].apply(lambda x: unidecode(x))\n",
    "\n",
    "# 8. remove blocks of numbers\n",
    "df[\"text_cleaned\"] = df[\"text_cleaned\"].str.replace(\"\\d+\", \" \", regex=True)\n",
    "\n",
    "# 9. remove extra white spaces before after sentences, as well as between words\n",
    "df[\"text_cleaned\"] = df[\"text_cleaned\"].str.replace(\"\\s+\", \" \", regex=True)\n",
    "\n",
    "# 10. remove misc bad letters or invalid letter blocks\n",
    "df[\"text_cleaned\"] = df[\"text_cleaned\"].str.replace(\"u_\", \" \")\n",
    "\n",
    "# 11. finally stripe all leading and ending whitespaces and numbers\n",
    "df[\"text_cleaned\"] = df[\"text_cleaned\"].str.strip(\"123.!? \\n\\t\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the above 9 simple data cleaning, most of our tweets are free of invalid characters and text strings. There are still two issues remain:\n",
    "\n",
    "* **misspelling of words**: a number of words provided in the dataset had been misspelled. Without correcting word spelling, the misspelled words will likely be either discarded during feature engineering and selection, and have little predictive power if included.\n",
    "* **retweets and nearly identical tweets**: identical tweets and retweets can bias the dataset toward a specific set of words and phrases. It's better if identical copy or very similar tweets are removed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # To fix mispelling of words, we cab use jaccard_distance.\n",
    "# def sentenceSpellCorrect(sentence):\n",
    "\n",
    "#     # credit: https://www.geeksforgeeks.org/correcting-words-using-nltk-in-python/\n",
    "#     # list of incorrect spellings\n",
    "#     # that need to be corrected\n",
    "#     check_words = sentence.split(\" \")\n",
    "#     new_words = []\n",
    "\n",
    "#     # loop for finding correct spellings\n",
    "#     # based on jaccard distance\n",
    "#     # and printing the correct word\n",
    "#     for word in check_words:\n",
    "#         try:\n",
    "#             temp = [\n",
    "#                 (jaccard_distance(set(ngrams(word, 2)), set(ngrams(w, 2))), w)\n",
    "#                 for w in correct_words\n",
    "#                 if w[0] == word[0]\n",
    "#             ]\n",
    "#             new = sorted(temp, key=lambda val: val[0])[0][1]\n",
    "#             if word != new:\n",
    "#                 print(f\"{word} has been corrected to {new}\")\n",
    "#             new_words.append(new)\n",
    "\n",
    "#         except:\n",
    "#             print(f\"error occurred when processing sentence: '{sentence}'\")\n",
    "#     new_sentence = \" \".join(new_words)\n",
    "#     return new_sentence\n",
    "\n",
    "\n",
    "# global correct_words\n",
    "# correct_words = words.words()\n",
    "# df[\"text_cleaned\"] = df[\"text_cleaned\"].apply(lambda x: sentenceSpellCorrect(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "at the end of data cleaning, separate test and train datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>train</th>\n",
       "      <th>text_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>our deeds are the reason of this earthquake ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>all residents asked to 'shelter in place' are ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>people receive wildfires evacuation orders in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>just got sent this photo from ruby alaska as s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  train                                       text_cleaned  \n",
       "0     1.0      1  our deeds are the reason of this earthquake ma...  \n",
       "1     1.0      1              forest fire near la ronge sask canada  \n",
       "2     1.0      1  all residents asked to 'shelter in place' are ...  \n",
       "3     1.0      1  people receive wildfires evacuation orders in ...  \n",
       "4     1.0      1  just got sent this photo from ruby alaska as s...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = df[df.train == 1]\n",
    "df_train.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>train</th>\n",
       "      <th>text_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>heard about earthquake is different cities sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>there is a forest fire at spot pond geese are ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>apocalypse lighting spokane wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>typhoon soudelor kills in china and taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash   \n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...   \n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...   \n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires   \n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan   \n",
       "\n",
       "   target  train                                       text_cleaned  \n",
       "0     NaN      0                 just happened a terrible car crash  \n",
       "1     NaN      0  heard about earthquake is different cities sta...  \n",
       "2     NaN      0  there is a forest fire at spot pond geese are ...  \n",
       "3     NaN      0              apocalypse lighting spokane wildfires  \n",
       "4     NaN      0         typhoon soudelor kills in china and taiwan  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = df[df.train == 0]\n",
    "df_test.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the rest of the project, we will no longer be using df_test, so set df_train to df\n",
    "df = df_train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Bag of words model\n",
    "\n",
    "A bag of words model is a simple method of processing text data where a string of text (document) is converted to an array of words. These words are then insert into a collection with no specific order. A count of the words by category can reveal what type of words are most frequently occurring by category.\n",
    "\n",
    "The bag of words is also known as unigrams and can be used as features into predictive models for classification and clustering. In our exercise, we ran through the full set of unigrams generated with a simple feature selection for low sample size (1 occurrence only) and with a variance threshold of 0.005. This removes a number of features that provide little information and differentiation in our model, and prevents overfitting.\n",
    "\n",
    "Finally, a simple logistic regression model is used to model the label for disaster tweets. The default `L2` penalty term is used so we can use the `lbfgs` solver for faster model convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1A. Build bag of words / unigrams (Feature Engineering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through the entire training df and build bag of words (tokens) for target == 1 and target == 0\n",
    "\n",
    "bag_target_0 = []\n",
    "for sentence in list(df[df.target == 0][\"text_cleaned\"]):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    bag_target_0 = bag_target_0 + tokens\n",
    "\n",
    "bag_target_1 = []\n",
    "for sentence in list(df[df.target == 1][\"text_cleaned\"]):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    bag_target_1 = bag_target_1 + tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform preprocessing to further clean the word list\n",
    "\n",
    "# three main preprocessing are done on tokens or word list\n",
    "# 1. remove non alphanumeric characters or symbols that slip through data cleaning\n",
    "# 2. remove any stop words like the, a, etc. that are present for grammatical reasons and are semantically meaningful\n",
    "# 3. convert similar words together via Lemmatization, such that the base word is used\n",
    "wordnet = WordNetLemmatizer()\n",
    "\n",
    "bag_target_0_processed = bag_target_0\n",
    "bag_target_0_processed = [i for i in bag_target_0_processed if i.isalpha()]\n",
    "bag_target_0_processed = [\n",
    "    i for i in bag_target_0_processed if i not in stopwords.words(\"english\")\n",
    "]\n",
    "bag_target_0_processed = [wordnet.lemmatize(i) for i in bag_target_0_processed]\n",
    "count_bag_target_0_processed = collections.Counter(bag_target_0_processed)\n",
    "# count_bag_target_0_processed.most_common(10)\n",
    "\n",
    "bag_target_1_processed = bag_target_1\n",
    "bag_target_1_processed = [i for i in bag_target_1_processed if i.isalpha()]\n",
    "bag_target_1_processed = [\n",
    "    i for i in bag_target_1_processed if i not in stopwords.words(\"english\")\n",
    "]\n",
    "bag_target_1_processed = [wordnet.lemmatize(i) for i in bag_target_1_processed]\n",
    "count_bag_target_1_processed = collections.Counter(bag_target_1_processed)\n",
    "# count_bag_target_1_processed.most_common(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute frequency of bag of words\n",
    "\n",
    "count_bag_target_0_processed = dict(\n",
    "    zip(count_bag_target_0_processed.keys(), count_bag_target_0_processed.values())\n",
    ")\n",
    "count_bag_target_1_processed = dict(\n",
    "    zip(count_bag_target_1_processed.keys(), count_bag_target_1_processed.values())\n",
    ")\n",
    "\n",
    "df_bag_target_0 = pd.DataFrame(\n",
    "    list(count_bag_target_0_processed.items()), columns=[\"word\", \"count\"]\n",
    ")\n",
    "df_bag_target_1 = pd.DataFrame(\n",
    "    list(count_bag_target_1_processed.items()), columns=[\"word\", \"count\"]\n",
    ")\n",
    "\n",
    "df_bag_target_0[\"freq\"] = df_bag_target_0[\"count\"] / df_bag_target_0[\"count\"].sum()\n",
    "df_bag_target_1[\"freq\"] = df_bag_target_1[\"count\"] / df_bag_target_1[\"count\"].sum()\n",
    "\n",
    "df_bag_target_0 = df_bag_target_0.sort_values(by=\"count\", ascending=False)\n",
    "df_bag_target_0 = df_bag_target_0.reset_index(drop=True)\n",
    "df_bag_target_1 = df_bag_target_1.sort_values(by=\"count\", ascending=False)\n",
    "df_bag_target_1 = df_bag_target_1.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "review word count of 10 most common words for non-disaster tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>count</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>like</td>\n",
       "      <td>256</td>\n",
       "      <td>0.007152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>u</td>\n",
       "      <td>214</td>\n",
       "      <td>0.005978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>get</td>\n",
       "      <td>184</td>\n",
       "      <td>0.005140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>new</td>\n",
       "      <td>170</td>\n",
       "      <td>0.004749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>one</td>\n",
       "      <td>138</td>\n",
       "      <td>0.003855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>body</td>\n",
       "      <td>117</td>\n",
       "      <td>0.003269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>time</td>\n",
       "      <td>106</td>\n",
       "      <td>0.002961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>would</td>\n",
       "      <td>105</td>\n",
       "      <td>0.002933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>day</td>\n",
       "      <td>104</td>\n",
       "      <td>0.002905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>video</td>\n",
       "      <td>102</td>\n",
       "      <td>0.002849</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    word  count      freq\n",
       "0   like    256  0.007152\n",
       "1      u    214  0.005978\n",
       "2    get    184  0.005140\n",
       "3    new    170  0.004749\n",
       "4    one    138  0.003855\n",
       "5   body    117  0.003269\n",
       "6   time    106  0.002961\n",
       "7  would    105  0.002933\n",
       "8    day    104  0.002905\n",
       "9  video    102  0.002849"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bag_target_0.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "review word count of 10 most common words for disaster tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>count</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fire</td>\n",
       "      <td>266</td>\n",
       "      <td>0.008747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>u</td>\n",
       "      <td>162</td>\n",
       "      <td>0.005327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>news</td>\n",
       "      <td>148</td>\n",
       "      <td>0.004867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>disaster</td>\n",
       "      <td>122</td>\n",
       "      <td>0.004012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>via</td>\n",
       "      <td>121</td>\n",
       "      <td>0.003979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>year</td>\n",
       "      <td>112</td>\n",
       "      <td>0.003683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>california</td>\n",
       "      <td>111</td>\n",
       "      <td>0.003650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>police</td>\n",
       "      <td>109</td>\n",
       "      <td>0.003584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>suicide</td>\n",
       "      <td>106</td>\n",
       "      <td>0.003486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>family</td>\n",
       "      <td>105</td>\n",
       "      <td>0.003453</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         word  count      freq\n",
       "0        fire    266  0.008747\n",
       "1           u    162  0.005327\n",
       "2        news    148  0.004867\n",
       "3    disaster    122  0.004012\n",
       "4         via    121  0.003979\n",
       "5        year    112  0.003683\n",
       "6  california    111  0.003650\n",
       "7      police    109  0.003584\n",
       "8     suicide    106  0.003486\n",
       "9      family    105  0.003453"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bag_target_1.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary of BOW results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of the top 10 most common words, we see more words related to disasters like *fire*, *disaster*, *police*, etc in the disaster bag, and more neutral and non disaster words like *get*, *time*, and *would*. From the common words, we can also identify words that are common in both but is generally not very meaningful like *u*, *year*, but occurs frequently. These common words in both bags can introduce confusion for a classification model on predicting disaster tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1B. Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove low sample rate features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to start, remove low occurrence words\n",
    "# these words occurs so rarely that they have little predictive power\n",
    "\n",
    "bag_words = bag_target_0_processed + bag_target_1_processed\n",
    "count_bag_words = collections.Counter(bag_words)\n",
    "\n",
    "count_bag_words = dict(zip(count_bag_words.keys(), count_bag_words.values()))\n",
    "df_bag_words = pd.DataFrame(list(count_bag_words.items()), columns=[\"word\", \"count\"])\n",
    "LOW_OCCURRENCE_THRESHOLD = 1\n",
    "df_bag_words = df_bag_words[df_bag_words[\"count\"] > LOW_OCCURRENCE_THRESHOLD]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>train</th>\n",
       "      <th>text_cleaned</th>\n",
       "      <th>_man</th>\n",
       "      <th>_love</th>\n",
       "      <th>_fruit</th>\n",
       "      <th>...</th>\n",
       "      <th>_idfire</th>\n",
       "      <th>_scariest</th>\n",
       "      <th>_ekiti</th>\n",
       "      <th>_salvadoran</th>\n",
       "      <th>_exchanging</th>\n",
       "      <th>_exc</th>\n",
       "      <th>_fatally</th>\n",
       "      <th>_ushed</th>\n",
       "      <th>_kashmir</th>\n",
       "      <th>_woodlawn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>our deeds are the reason of this earthquake ma...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>all residents asked to 'shelter in place' are ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>people receive wildfires evacuation orders in ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>just got sent this photo from ruby alaska as s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 5633 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  train                                       text_cleaned  _man  \\\n",
       "0     1.0      1  our deeds are the reason of this earthquake ma...     0   \n",
       "1     1.0      1              forest fire near la ronge sask canada     0   \n",
       "2     1.0      1  all residents asked to 'shelter in place' are ...     0   \n",
       "3     1.0      1  people receive wildfires evacuation orders in ...     0   \n",
       "4     1.0      1  just got sent this photo from ruby alaska as s...     0   \n",
       "\n",
       "   _love  _fruit  ...  _idfire  _scariest  _ekiti  _salvadoran  _exchanging  \\\n",
       "0      0       0  ...        0          0       0            0            0   \n",
       "1      0       0  ...        0          0       0            0            0   \n",
       "2      0       0  ...        0          0       0            0            0   \n",
       "3      0       0  ...        0          0       0            0            0   \n",
       "4      0       0  ...        0          0       0            0            0   \n",
       "\n",
       "   _exc  _fatally  _ushed  _kashmir  _woodlawn  \n",
       "0     0         0       0         0          0  \n",
       "1     0         0       0         0          0  \n",
       "2     0         0       0         0          0  \n",
       "3     0         0       0         0          0  \n",
       "4     0         0       0         0          0  \n",
       "\n",
       "[5 rows x 5633 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build training dataset using bag of words\n",
    "def buildFeaturesFromWords(df, df_text_col, word_list):\n",
    "    df = df.copy()\n",
    "    series_list = []\n",
    "    for word in word_list:\n",
    "        # count number of this word in the sentence text\n",
    "        if len(word) > 1:\n",
    "            # discard single letter words\n",
    "            s = df[df_text_col].str.count(word).rename(f\"_{word}\")\n",
    "            series_list.append(s)\n",
    "    return pd.concat([df, pd.concat(series_list, axis=1)], axis=1)\n",
    "\n",
    "\n",
    "unique_words = list(df_bag_words.word.unique())\n",
    "# make sure the unique word list is smaller than separate bags\n",
    "assert (len(df_bag_target_0) + len(df_bag_target_1)) > len(unique_words)\n",
    "\n",
    "df = buildFeaturesFromWords(df=df, df_text_col=\"text_cleaned\", word_list=unique_words)\n",
    "\n",
    "# build a list of variable names that correspond to df columns\n",
    "unigram_list = [f\"_{i}\" for i in unique_words if len(i) > 1]\n",
    "\n",
    "df.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove variables based on variance threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_var = \"target\"\n",
    "X, y = df[unigram_list], df_train[pred_var]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "by applying a low variance threshold of 0.005, 4668 out of 5626 variables were removed\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "LOW_VAR_THRESHOLD = 0.005\n",
    "sel = VarianceThreshold(threshold=LOW_VAR_THRESHOLD)\n",
    "sel.fit(X)\n",
    "# print(sel.get_feature_names_out())\n",
    "print(\n",
    "    f\"by applying a low variance threshold of {LOW_VAR_THRESHOLD}, {len(unigram_list) - len(sel.get_feature_names_out())} out of {len(unigram_list)} variables were removed\"\n",
    ")\n",
    "\n",
    "selected_unigram_list = list(sel.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_var = \"target\"\n",
    "X, y = df[selected_unigram_list], df_train[pred_var]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1C. Logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, random_state=0, train_size=0.8\n",
    ")\n",
    "reg = linear_model.LogisticRegression(solver=\"lbfgs\", max_iter=1000)\n",
    "reg.fit(X_train, y_train)\n",
    "y_pred = reg.predict(X_test)\n",
    "print(f\"score: {accuracy_score(y_test, y_pred)}\")\n",
    "print(classification_report(y_test, y_pred, target_names=[\"disaster\", \"normal\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_within = reg.predict(X_train)\n",
    "cm = confusion_matrix(y_train, y_pred_within)\n",
    "\n",
    "print(f\"score(within sample): {accuracy_score(y_train, y_pred_within)}\")\n",
    "print(classification_report(y_train, y_pred_within, target_names=[\"disaster\", \"normal\"]))\n",
    "\n",
    "print(\"Confusion matrix format:\\n [['TP', 'FP'],\\n ['FN', 'TN']]\")\n",
    "print(\"Confusion matrix for within sample predictions (percentage):\")\n",
    "print(cm / cm.sum() * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(\"Confusion matrix for out of sample predictions\")\n",
    "print(cm / cm.sum() * 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simple logistic regression model using bag of words features performed reasonably well. However, the confusion matrix for out of sample prediction is noticeably worse than within sample prediction. This can be an indication of overfitting.\n",
    "\n",
    "Additional effort in using cross validation methods (k-fold), and regularization method (ridge classification) that is more robust to overfitting can reduce model overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  evaluate and plot the model\n",
    "\n",
    "df.loc[df[\"target\"] == 0, \"target_name\"] = \"Normal\"\n",
    "df.loc[df[\"target\"] == 1, \"target_name\"] = \"Disaster\"\n",
    "\n",
    "# generate probability for all samples for plotting\n",
    "y_pred_all = reg.predict_proba(X)\n",
    "df[\"prob_1\"] = y_pred_all[:, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code block has error\n",
    "\n",
    "# from scattertext import CorpusFromPandas, produce_scattertext_explorer\n",
    "\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "# corpus = st.CorpusFromPandas(\n",
    "#     df.sample(1000), category_col=\"target_name\", text_col=\"text_cleaned\", nlp=nlp\n",
    "# ).build()\n",
    "\n",
    "# html = st.produce_scattertext_explorer(\n",
    "#     corpus,\n",
    "#     category=\"target_name\",\n",
    "#     category_name=\"Disaster\",\n",
    "#     not_category_name=\"Normal\",\n",
    "#     width_in_pixels=1000,\n",
    "#     minimum_term_frequency=0,\n",
    "#     transform=st.Scalers.scale,\n",
    "#     # metadata=df['speaker']\n",
    "# )\n",
    "# file_name = \"Conventions2012ScattertextScale.html\"\n",
    "# open(file_name, \"wb\").write(html.encode(\"utf-8\"))\n",
    "# IFrame(src=file_name, width=1200, height=700)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text_short\"] = df[\"text_cleaned\"].str.slice(0, 6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sns.stripplot(df, x=\"text_short\", y=\"prob_1\", hue=\"target_name\", size=1.5)\n",
    "# fig.set(xlabel=None)\n",
    "fig.set(xticklabels=[])\n",
    "fig.tick_params(bottom=False)\n",
    "plt.axhline(y=0.5, color=\"red\", linestyle=\"--\")\n",
    "plt.legend(loc=\"upper right\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are still a number of misclassification with the simple Logistic Regression model with bag of words.\n",
    "\n",
    "We hope to improve upon this method by accounting for the importance of words in the dataset using Term Frequency and Inverse Document Frequency (TF-IDF), in the next task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1D. Summary for Log1-BOW Model (Needs update)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision (TP / [TP + FP]) indicates the fraction of items labelled true in `target` that is actually true. In the **Log2-IDIEF** model, the positive predictive value of `normal` is *slightly higher* than `disaster`, indicating that the model has a bias in predicting normal. Generally, a higher precision could be concerning if we do not have resources to respond to every disaster predicted.\n",
    "\n",
    "Recall (TP / [TP + FN]), indicates the fraction of item labelled true that was originally true. The sensitivity of the model at predicting disaster is *much higher* than normal category. This indicate disaster prediction is unlikely to be a false negative. Generally, a higher recall is good if we do not want to miss any real disaster event tweets.\n",
    "\n",
    "In the confusion table below, we can see that the prediction for False Positive (FP) is *much lower* than False Negative (FN), and the confusion matrix values for FP and FN are both higher in the out of sample prediction. This is consistent with the precision and recall values we see in the classification report.\n",
    "\n",
    "Unfortunately, ngrams with unigrams and bigrams performed worse than the simple unigram model, model **Log1-BOW**. This could be due to the fact that most bigrams don't add enough useful information to each feature to justify the larger feature space, resulting in additional noise in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Feature generation and traditional ML model with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up dataframe to have the cleaned text, target, and full list of unigram\n",
    "df = df[[\"id\", \"keyword\", \"location\", \"text_cleaned\", \"target\"]]\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2A. Build ngrams with TF-IDF (Feature Engineering)\n",
    "\n",
    "In addition to the issue of weighting with bag of words, the use of unigram also means the feature lacks context. For Task 2 with TF-IDF, in addition to unigram, bigrams are generated as a set of features to enhance the ability of models to capture context.\n",
    "\n",
    "For task 2, the TfidfVectorizer from sklearn is used to generate both ngrams and the TF-IDF score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_About TF-IDF Score_\n",
    "\n",
    "TF = (Count of this word in the document) / (Total number of words in the document)\n",
    "IDF = log {(Total number of document) / [(Number of document containing the word) + 1]}\n",
    "\n",
    "TF-IDF = TF * IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from time import time\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_df=0.5,\n",
    "    min_df=5,\n",
    "    ngram_range=(1, 2),  # unigrams and bigrams\n",
    "    stop_words=\"english\",\n",
    ")\n",
    "t0 = time()\n",
    "X_tfidf = vectorizer.fit_transform(df.text_cleaned)\n",
    "\n",
    "print(f\"vectorization done in {time() - t0:.3f} s\")\n",
    "print(f\"n_samples: {X_tfidf.shape[0]}, n_features: {X_tfidf.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.get_feature_names_out()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prevent overfitting, we reduce the number of features by setting a higher low variance threshold for bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform feature selection\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "LOW_VAR_THRESHOLD = 0.0001\n",
    "sel = VarianceThreshold(threshold=LOW_VAR_THRESHOLD)\n",
    "sel.fit(X_tfidf)\n",
    "X_tfidf = sel.transform(X_tfidf)\n",
    "# print(sel.get_feature_names_out())\n",
    "print(\n",
    "    f\"by applying a low variance threshold of {LOW_VAR_THRESHOLD}, {len(vectorizer.get_feature_names_out()) - len(sel.get_feature_names_out())} out of {len(vectorizer.get_feature_names_out())} ngram variables were removed\"\n",
    ")\n",
    "\n",
    "selected_ngram_list = list(sel.get_feature_names_out())\n",
    "\n",
    "print(f\"length of selected_ngram_list: {len(selected_ngram_list)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2B. Logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_train[pred_var]\n",
    "\n",
    "X_tfidf.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_tfidf, y, random_state=0, train_size=0.8\n",
    ")\n",
    "reg = linear_model.LogisticRegression(solver=\"lbfgs\", max_iter=1000)\n",
    "reg.fit(X_train, y_train)\n",
    "y_pred = reg.predict(X_test)\n",
    "print(f\"score: {accuracy_score(y_test, y_pred)}\")\n",
    "print(classification_report(y_test, y_pred, target_names=[\"disaster\", \"normal\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_within = reg.predict(X_train)\n",
    "cm = confusion_matrix(y_train, y_pred_within)\n",
    "\n",
    "print(f\"score(within sample): {accuracy_score(y_train, y_pred_within)}\")\n",
    "print(classification_report(y_train, y_pred_within, target_names=[\"disaster\", \"normal\"]))\n",
    "\n",
    "print(\"Confusion matrix format:\\n [['TP', 'FP'],\\n ['FN', 'TN']]\")\n",
    "print(\"Confusion matrix for within sample predictions (percentage):\")\n",
    "print(cm / cm.sum() * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(\"Confusion matrix for out of sample predictions\")\n",
    "print(cm / cm.sum() * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  evaluate and plot the model\n",
    "\n",
    "df.loc[df[\"target\"] == 0, \"target_name\"] = \"Normal\"\n",
    "df.loc[df[\"target\"] == 1, \"target_name\"] = \"Disaster\"\n",
    "\n",
    "# generate probability for all samples for plotting\n",
    "y_pred_all = reg.predict_proba(X_tfidf)\n",
    "df[\"prob_1\"] = y_pred_all[:, 1]\n",
    "\n",
    "\n",
    "fig = sns.stripplot(df, x=\"text_cleaned\", y=\"prob_1\", hue=\"target_name\", size=1.5)\n",
    "# fig.set(xlabel=None)\n",
    "fig.set(xticklabels=[])\n",
    "fig.tick_params(bottom=False)\n",
    "plt.axhline(y=0.5, color=\"red\", linestyle=\"--\")\n",
    "plt.legend(loc=\"upper right\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2C. Summary for Log1-IDIEF Model (Needs update)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision (TP / [TP + FP]) indicates the fraction of items labelled true in `target` that is actually true. In the **Log2-IDIEF** model, the positive predictive value of `normal` is *slightly higher* than `disaster`, indicating that the model has a bias in predicting normal. Generally, a higher precision could be concerning if we do not have resources to respond to every disaster predicted.\n",
    "\n",
    "Recall (TP / [TP + FN]), indicates the fraction of item labelled true that was originally true. The sensitivity of the model at predicting disaster is *much higher* than normal category. This indicate disaster prediction is unlikely to be a false negative. Generally, a higher recall is good if we do not want to miss any real disaster event tweets.\n",
    "\n",
    "In the confusion table below, we can see that the prediction for False Positive (FP) is *much lower* than False Negative (FN), and the confusion matrix values for FP and FN are both higher in the out of sample prediction. This is consistent with the precision and recall values we see in the classification report.\n",
    "\n",
    "Unfortunately, ngrams with unigrams and bigrams performed worse than the simple unigram model, model **Log1-BOW**. This could be due to the fact that most bigrams don't add enough useful information to each feature to justify the larger feature space, resulting in additional noise in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D. Repeat experiment with only Unigram and TF-IDF, and logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from time import time\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_df=0.5,\n",
    "    min_df=5,\n",
    "    ngram_range=(1, 1),  # unigrams only\n",
    "    stop_words=\"english\",\n",
    ")\n",
    "t0 = time()\n",
    "X_tfidf = vectorizer.fit_transform(df.text_cleaned)\n",
    "\n",
    "print(f\"vectorization done in {time() - t0:.3f} s\")\n",
    "print(f\"n_samples: {X_tfidf.shape[0]}, n_features: {X_tfidf.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_tfidf, y, random_state=0, train_size=0.8\n",
    ")\n",
    "reg = linear_model.LogisticRegression(solver=\"lbfgs\", max_iter=1000)\n",
    "reg.fit(X_train, y_train)\n",
    "y_pred = reg.predict(X_test)\n",
    "print(f\"score: {accuracy_score(y_test, y_pred)}\")\n",
    "print(classification_report(y_test, y_pred, target_names=[\"disaster\", \"normal\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_within = reg.predict(X_train)\n",
    "cm = confusion_matrix(y_train, y_pred_within)\n",
    "\n",
    "print(f\"score(within sample): {accuracy_score(y_train, y_pred_within)}\")\n",
    "print(classification_report(y_train, y_pred_within, target_names=[\"disaster\", \"normal\"]))\n",
    "\n",
    "print(\"Confusion matrix format:\\n [['TP', 'FP'],\\n ['FN', 'TN']]\")\n",
    "print(\"Confusion matrix for within sample predictions (percentage):\")\n",
    "print(cm / cm.sum() * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(\"Confusion matrix for out of sample predictions\")\n",
    "print(cm / cm.sum() * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  evaluate and plot the model\n",
    "\n",
    "df.loc[df[\"target\"] == 0, \"target_name\"] = \"Normal\"\n",
    "df.loc[df[\"target\"] == 1, \"target_name\"] = \"Disaster\"\n",
    "\n",
    "# generate probability for all samples for plotting\n",
    "y_pred_all = reg.predict_proba(X_tfidf)\n",
    "df[\"prob_1\"] = y_pred_all[:, 1]\n",
    "\n",
    "\n",
    "fig = sns.stripplot(df, x=\"text_cleaned\", y=\"prob_1\", hue=\"target_name\", size=1.5)\n",
    "# fig.set(xlabel=None)\n",
    "fig.set(xticklabels=[])\n",
    "fig.tick_params(bottom=False)\n",
    "plt.axhline(y=0.5, color=\"red\", linestyle=\"--\")\n",
    "plt.legend(loc=\"upper right\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2E. Summary for Log1-IDIEF Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision (TP / [TP + FP]) indicates the fraction of items labelled true in `target` that is actually true. In the **Log1-IDIEF** model, the positive predictive value of `normal` is *slightly higher* than `disaster`, indicating that the model has a bias in predicting normal. Generally, a higher precision could be concerning if we do not have resources to respond to every disaster predicted.\n",
    "\n",
    "Recall (TP / [TP + FN]), indicates the fraction of item labelled true that was originally true. The sensitivity of the model at predicting disaster is *much higher* than normal category. This indicate disaster prediction is unlikely to be a false negative. Generally, a higher recall is good if we do not want to miss any real disaster event tweets.\n",
    "\n",
    "In the confusion table below, we can see that the prediction for False Positive (FP) is *much lower* than False Negative (FN), and the confusion matrix values for FP and FN are both higher in the out of sample prediction. This is consistent with the precision and recall values we see in the classification report.\n",
    "\n",
    "Overall, the **Log1-IDIEF** model with unigram and tf-idf actually performed very similarly to the previous **Log2-IDIEF** model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Pre-trained word embeddings + linear classifier model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3A. Apply pre-trained GloVe embeddings to generate features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "# !unzip -q glove.6B.zip\n",
    "\n",
    "# credit to example: https://keras.io/examples/nlp/pretrained_word_embeddings/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"rand\"] = np.random.rand(len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_words = df[df.rand > 0.2].text_cleaned\n",
    "test_words = df[df.rand <= 0.2].text_cleaned\n",
    "train_labels = df[df.rand > 0.2].target\n",
    "test_labels = df[df.rand <= 0.2].target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_words.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # use our cleaned text to build a word index - this generates lots of misses because sentences have not been converted into word strings\n",
    "# word_index = dict(zip(list(df.text_cleaned), range(len(df))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to perform text vectorization, so we can map our sequence of words as tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "vectorizer = TextVectorization(max_tokens=20000, output_sequence_length=50)\n",
    "text_ds = tf.data.Dataset.from_tensor_slices(train_words).batch(128)\n",
    "vectorizer.adapt(text_ds)\n",
    "\n",
    "voc = vectorizer.get_vocabulary()\n",
    "word_index = dict(zip(voc, range(len(voc))))\n",
    "\n",
    "X_train = vectorizer(np.array([[s] for s in train_words])).numpy()\n",
    "X_test = vectorizer(np.array([[s] for s in test_words])).numpy()\n",
    "X_all = vectorizer(np.array([[s] for s in list(df.text_cleaned)])).numpy()\n",
    "\n",
    "y_train = np.array(train_labels)\n",
    "y_test = np.array(test_labels)\n",
    "\n",
    "vectorizer.get_vocabulary()[-5:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word index and voc length should be identical\n",
    "assert len(voc) == len(word_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read glove word embeddings\n",
    "\n",
    "path_to_glove_file = os.path.join(\"./data/glove.6B.100d.txt\")\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(path_to_glove_file) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index[\"a\"].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build an embedding matrix\n",
    "\n",
    "num_tokens = len(word_index) + 2\n",
    "embedding_dim = 100\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "# Prepare embedding matrix\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the prepared embedding matrix to create an embedding layer\n",
    "\n",
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(\n",
    "    num_tokens,\n",
    "    embedding_dim,\n",
    "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "    trainable=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply glove text embedding\n",
    "\n",
    "X_train = embedding_layer(X_train)\n",
    "X_test = embedding_layer(X_test)\n",
    "X_all = embedding_layer(X_all)\n",
    "\n",
    "# y_train = np.array(train_labels)\n",
    "# y_test = np.array(test_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reshape 50 x 100 word embedding features into 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_old_i0 = X_train[0]\n",
    "X_train_old_i0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tf.reshape(X_train, shape=(len(X_train), 5000))\n",
    "X_test = tf.reshape(X_test, shape=(len(X_test), 5000))\n",
    "X_all = tf.reshape(X_all, shape=(len(X_all), 5000))\n",
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check reshape is done correctly\n",
    "(tf.reshape(X_train[0], (50, 100)) == X_train_old_i0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We do not need to apply a full keras model\n",
    "\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense\n",
    "# from keras.layers import Flatten\n",
    "\n",
    "# vocab_size = 20000\n",
    "# max_length = 50\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(vocab_size, 8, input_length=max_length))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(1, activation=\"sigmoid\"))\n",
    "# model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "# print(model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3B. Stochastic gradient descent with glove word embeddings (SGD-Glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "reg = linear_model.SGDClassifier(\n",
    "    loss=\"modified_huber\", penalty=\"l2\", max_iter=1000, tol=0.001\n",
    ")\n",
    "reg.fit(X_train, y_train)\n",
    "y_pred = reg.predict(X_test)\n",
    "print(f\"score: {accuracy_score(y_test, y_pred)}\")\n",
    "print(classification_report(y_test, y_pred, target_names=[\"disaster\", \"normal\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_within = reg.predict(X_train)\n",
    "cm = confusion_matrix(y_train, y_pred_within)\n",
    "\n",
    "print(f\"score(within sample): {accuracy_score(y_train, y_pred_within)}\")\n",
    "print(classification_report(y_train, y_pred_within, target_names=[\"disaster\", \"normal\"]))\n",
    "\n",
    "print(\"Confusion matrix format:\\n [['TP', 'FP'],\\n ['FN', 'TN']]\")\n",
    "print(\"Confusion matrix for within sample predictions (percentage):\")\n",
    "print(cm / cm.sum() * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(\"Confusion matrix for out of sample predictions\")\n",
    "print(cm / cm.sum() * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  evaluate and plot the model\n",
    "\n",
    "df.loc[df[\"target\"] == 0, \"target_name\"] = \"Normal\"\n",
    "df.loc[df[\"target\"] == 1, \"target_name\"] = \"Disaster\"\n",
    "\n",
    "# generate probability for all samples for plotting\n",
    "y_pred_all = reg.predict_proba(X_all)\n",
    "df[\"prob_1\"] = y_pred_all[:, 1]\n",
    "\n",
    "\n",
    "fig = sns.stripplot(df, x=\"text_cleaned\", y=\"prob_1\", hue=\"target_name\", size=1.5)\n",
    "# fig.set(xlabel=None)\n",
    "fig.set(xticklabels=[])\n",
    "fig.tick_params(bottom=False)\n",
    "plt.axhline(y=0.5, color=\"red\", linestyle=\"--\")\n",
    "plt.legend(loc=\"upper right\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3C. Summary for SGD-Glove model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision (TP / [TP + FP]) indicates the fraction of items labelled true in `target` that is actually true. In the **SGD-Glove** model, the positive predictive value of `normal` is *lower* than `disaster`, indicating that the model has a bias in predicting disaster.\n",
    "\n",
    "Recall (TP / [TP + FN]), indicates the fraction of item labelled true that was originally true. The sensitivity of the model at predicting disaster and normal is fairly *similar*.\n",
    "\n",
    "In the confusion table below, we can see that the prediction for False Positive (FP) is *slightly higher* than False Negative (FN), and the confusion matrix values for FP and FN are both *higher* in the out of sample prediction. This is consistent with the precision and recall values we see in the classification report.\n",
    "\n",
    "Overall, the **SGD-Glove** model performed worse than the previous models. This is especially notable in the difference in accuracy and f1-score between the test and train sample. This is an indication of overfitting due to a large number of feature generated using the Glove pre-trained text embedding. Model more robust to overfitting like ensemble models (gradient boosted trees, and random forest) could be explored with the Glove text embedding in the future.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendation and Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eb8fb7b32aa2cdc561606137635a9a2f1ddd09c65d5c7a8cb8223afa1812e966"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
